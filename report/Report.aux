\relax 
\citation{welling2011bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {1}Preface}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}The Gibbs distribution}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Stochastic gradient Langevin dynamics}{1}{}\protected@file@percent }
\newlabel{SGLD}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Showing equivalence}{1}{}\protected@file@percent }
\bibdata{bib}
\bibcite{welling2011bayesian}{1}
\bibstyle{plain}
\@writefile{toc}{\contentsline {section}{\numberline {5}Simulated annealing}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments}{2}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The dataset in question. Blue points correspond to label 0 and orange points correspond to label 1.}}{2}{}\protected@file@percent }
\newlabel{swiss_roll}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces This figure shows that standard MBGD does not manage to properly learn a good solution to a relatively simple classification problem, while noisy MBGD finds a very strong solution}}{2}{}\protected@file@percent }
\newlabel{loss}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The final output after training using noisy MBGD}}{3}{}\protected@file@percent }
\newlabel{noisy_sol}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The final output after training using vanilla MBGD}}{3}{}\protected@file@percent }
\newlabel{van_sol}{{4}{3}}
\gdef \@abspage@last{3}
